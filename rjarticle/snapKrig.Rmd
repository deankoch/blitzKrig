---
title: "snapKrig: An R Package for Fast Geostatistics with Kronecker Covariances"
abstract: >
  We introduce a new R package, called \CRANpkg{snapKrig}, for modelling two-dimensional stationary Gaussian Processes. The package assumes gridded data and a special covariance structure where the joint covariance matrix V can factored using Kronecker products. This scheme implicitly supports both isotropic and anisotropic random fields, and it greatly simplifies several important modelling equations, including the likelihood function, generalized least squares, and the kriging predictor and variance equations. Our package offers computationally lean implementations of these functions, and much more. We demonstrate kriging functionality with predictions on the Meuse soils dataset, and also report on a benchmark experiment for computation time that shows improvements of several orders of magnitude compared to three major alternative R packages for spatial kriging. 
  
draft: false
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author:  
  # see ?rjournal_article for more information
  - name: Dean Koch
    affiliation: University of Alberta
    address:
    - Department of Mathematical and Statistical Sciences
    - 11324 89 Ave NW, Edmonton, AB, Canada, T6G 2J5.
    url: https://github.com/deankoch/snapKrig
    orcid: 0000-0002-8849-859X
    email:  dkoch@ualberta.ca
  - name: Subhash Lele
    affiliation: University of Alberta
    address:
    - Department of Mathematical and Statistical Sciences
    - 11324 89 Ave NW, Edmonton, AB, Canada, T6G 2J5.
    url: 
    orcid: 
    email:  slele@ualberta.ca
  - name: Robert Crabtree
    affiliation: Yellowstone Ecological Research Center
    address:
    - Bozeman, MT
    url: 
    orcid: 
    email:  crabtree@yellowstoneresearch.org    
header-includes:
  - \usepackage{longtable}
  - \usepackage{booktabs}
type: package
output: 
  rjtools::rjournal_pdf_article:
    self_contained: false
    toc: false
    keep_tex: true
    pandoc_args: --natbib
bibliography: snapKrig
---

```{r setup, include=FALSE}
knitr::opts_chunk[['set']](echo = FALSE, warning = FALSE, message = FALSE)
# Try adding \usepackage{float} with chunk option fig.pos='h' if floating figures are driving you crazy.

# spatial libraries
library(sp)
library(sf)
library(terra)
library(snapKrig)

# directory management
library(here)

# define paths to test result files
dir_project = here('rjarticle')
dir_storage = file.path(dir_project, 'data')
path_cv_result = file.path(dir_storage, 'cv_results.csv')
path_bench_result = file.path(dir_storage, 'bench_results.csv')
path_dem = file.path(dir_storage, 'treed_dem.tif')
path_treed = file.path(dir_storage, 'treed.tif')

# load helpers and prepared datasets
source(file.path(dir_storage, 'data_helpers.R'))
meuse_list = get_meuse(dfMaxLength=NA)

# copy the log zinc points data
pts = meuse_list[['soils']]['log_zinc']

# save default graphical parameters
.par_default = par(no.readonly=TRUE)

# set up a common color palette (this is the default in snapKrig)
.pal = function(n) { hcl.colors(n, 'Spectral', rev=TRUE) }

library(smoothr)
library(ggplot2)

```

# Introduction

The second-order stationary Gaussian Process (SGP) model plays a central role in modern analysis of spatial data. It underlies several important techniques of inference and prediction in geostatistics [@chiles2012geostatistics], including generalized least squares and kriging. We see it also in engineering and computer experiments with surrogate models [@gramacy2016lagp], in machine learning [@rasmussen2006gaussian], and in probabilistic models for ecological systems [@koch2021signature], among many other applications.

In the SGP model, the joint probability distribution for $n$ points $z_k$, with coordinates $x_k$, $y_k$, is multivariate Gaussian and its variance-covariance matrix $V$ is generated by a function $C$ of the separation vector between points $V_{ij} = C ( x_i - x_j, y_i - y_j )$ [@cressie2015statistics]. This explicit formulation of $V$ is one of the charms of SGP theory, as it leads to simple analytic expressions for things like likelihood, conditional expectation, and least squares, all of which involve $V$ and its sub-matrices.

However, $V$ is a major headache for computer programmers. The number of entries in this matrix is $n^2$, so as sample sizes grow large, seemingly routine matrix expressions become surprisingly difficult to evaluate. For large enough $n$ readers will find that either $V$ is too large to fit in computer memory or the order-$n^3$ complexity of its factorization is prohibitively slow [@lindgren2011explicit].

These are major obstacles to software implementations of SGP, particularly when it comes to kriging. Kriging workflows with popular packages like \CRANpkg{gstat} [@pebesma2004gstat; @bivand2013applied], \CRANpkg{geoR}, [@ribeiro1999splus; @diggle2007model] and \CRANpkg{fields} [@nychka2021fields] can become so slow as to be unworkable, or even fail entirely due to out-of-memory errors, when the number of point locations of interest are large in number. 

This practical upper limit on sample sizes motivates us to introduce a new package, \CRANpkg{snapKrig}, whose implementation of the SGP is customized for Kronecker covariances on grids. These models are extremely fast compared to conventional alternatives. The computational ceiling is still there, but it is extended much higher. \pkg{snapKrig} offers:  

* Optimized functions for likelihood, GLS, kriging, and simulation
* Flexible parametric covariance structures supporting anisotropy and measurement error
* Simple beginner-friendly workflows for down-scaling
* Compatibility with data objects from \CRANpkg{raster} [@hijmansraster], \CRANpkg{terra} [@hijmansterra], and \CRANpkg{sf} [@pebesma2018sf] packages

The likelihood function is particularly useful because it will enable modelers to adapt the methods in \pkg{snapKrig} as an engine for more complex numerical techniques, such as Bayesian MCMC [@flaxman2015fast] or maximum likelihood for nonlinear trend parameters. In fact, this package is built from code that was originally written for the latter problem, in a study on mountain pine beetle outbreaks using integro-difference equations [@koch2020computationally].

We illustrate the speed-up offered by \pkg{snapKrig} in the [Computations] section, with a comparison of computation times for likelihood and prediction. This follows the [Kriging Example] section, where we present a detailed demonstration of a universal kriging workflow on the Meuse soils dataset. First we review some related tools and introduce the modelling approach that makes \pkg{snapKrig} unique.

## Background

One strategy for computation with $V$ on large-$n$ problems is parallelization. The \pkg{bigGP} [@paciorek2015biggp] and the  \CRANpkg{laGP} [@gramacy2016lagp] packages, for example, distribute the task of block factorization to multiple linked processes. This can speed likelihood calculations by several orders of magnitude in large-$n$ problems. However, these packages have steep learning curves, and are intended for high-memory, many-core computers. \pkg{snapKrig} instead aims to be simple to learn and operate, with a memory footprint suitable for use on ordinary desktop computers, and no reliance on parallelization.

Most existing packages of this type make use of local approximations to the desired covariance function, with the goal of introducing sparsity in $V$ or its inverse (the precision matrix). Examples include covariance tapering and fixed rank kriging, as in the \CRANpkg{LatticeKrig} [@nychka2016latticekrig] and \CRANpkg{FRK} [@zammit2021frk] packages. Markov Random Field approximations [@lindgren2011explicit] are also popular [despite the interpretability drawbacks discussed by @wall2004close], as are Bayesian approximations like \CRANpkg{spBayes} [@finley2007spbayes; @finley2015spbayes], \pkg{laGP} [@gramacy2016lagp], and R-INLA [@lindgren2015bayesian]. 

The strategy in \pkg{snapKrig} is different in that it does *exact* computations of the likelihood function for both compact and non-compact (*ie.* long-tailed) covariance functions, including the very common Gaussian covariance function. The resulting dense covariance matrix $V$ would normally be a problem in computations, but our package does not rely on sparsity. 

Instead we use an algebraic shortcut that emerges when gridded data are paired with certain covariance models, called Kronecker covariances [@flaxman2015fast; @koch2020computationally; @drton2021existence]. This shortcut appears in many areas of SGP research, including auto-regression [@martin1979subclass], space-time models [@kyriakidis1999geostatistical; @genton2007separable], and Bayesian hierarchical models for both spatial problems [@wilson2014fast; @flaxman2015fast2] and aspatial ones [@stegle2011efficient; @ronneberg2021bayesynergy]. Kronecker covariances also appear in spatial machine learning applications [@rasmussen2006gaussian; @neal2012bayesian; @yang2015carte], where a more general class of fast spatial operators, known as kernels, takes the place of $C$.

Despite their elegant algebraic and computational properties [@van2000ubiquitous], we rarely see Kronecker covariances exploited in software for exact geostatistical inference techniques like kriging. To our knowledge, all existing CRAN R packages for exact kriging implement a more general covariance model that does not exploit Kronecker structure (even when it is implicitly guaranteed). \pkg{snapKrig} fills this gap by dealing exclusively in Kronecker covariances, for faster performance.

CRAN lists a number of alternatives for exact inference with geostatistical models [@bivand2022cran], but three stand out for their scope, maturity, and quality of documentation: The \pkg{gstat} package for variogram-based geostatistical modeling; The \pkg{geoR} package, which supports variogram, likelihood, and Bayesian techniques, and \pkg{fields}, a feature-rich interpolation package for SGPs and spline models. In the [Computations] section we compare the most recent major release of these three packages with \pkg{snapKrig}.

\pkg{snapKrig} is a model for grid data. All observations, and all locations of interest for inference and prediction, must lie on the same regular grid. Many important datasets arrive in this form naturally, including: satellite imagery, digital elevation models (DEMs) and other remote sensing products [@cracknell1998review]; model outputs such as numerical weather prediction grids and maps of ecological parameters [@beaudoin2018tracking]; and grid-based transect surveys [@foster2020spatially].

Point data that are not gridded can be made so by simply defining a grid and translating the coordinates of each point to the closest grid point. *Snapping* points in this way introduces positional error, but this error can be made arbitrarily small, as we control the grid resolution.

## Model

Grid point values $z_k$ (for $k=1,\dots n$) are modelled as an observation of the $n$-dimensional random vector $Z \sim \text{N} \left( X\beta, V \right)$. This splits $z$ into a deterministic trend component and a random spatial component.  The trend is represented by the expected value $X\beta$, where $X$ is a known covariate data matrix and $\beta$ an unknown vector of coefficients. The spatial component is represented by a 2-dimensional covariance function $C$ which generates the covariance matrix $V$.

The covariance function is built from the product of two one-dimensional correlograms, $c_x$ and $c_y$, 
\begin{equation}
C \left( \Delta_x, \Delta_y \right) = \sigma^2 c_x \left( \Delta_x \right) c_y \left( \Delta_y \right) + \epsilon 1_{ \{ \Delta_x = \Delta_y = 0 \}},
(\#eq:covfun)
\end{equation}

where the component distances, $\Delta_x$ and $\Delta_y$, are scaled by range parameters $\rho_x$ and $\rho_y$,
\begin{equation}
\Delta_x = \frac{ \lvert x_i - x_j \rvert }{ \rho_x } \quad \text{and} \quad \Delta_y = \frac{ \lvert y_i - y_j \rvert }{ \rho_y }. (\#eq:dfun)
\end{equation}


```{r cfun-prep}
# define column names and a caption for the (two) calls below that make the table for the html and latex outputs
cfun_colnames = c('code', 'name', 'alias', '$c\\left( \\Delta \\right)$')
cfun_caption = 'A list of one-dimensional correlation functions available in snapKrig. Kronecker covariances are constructed from the product of a pair of these functions, one receiving the $x$ separation distance as its argument, and the other the $y$ distance. Normalization constants are omitted for brevity, and $K_p$ denotes the order-$p$ Bessel function of the second kind (where $p$ is a shape parameter).'

# define the table
cfun_df = data.frame(

  code = c('exp', 'gau', 'gex',  'mat', 'sph'),
  name = c('Exponential', 'Gaussian', 'Gamma-Exponential', "Mat\\'ern", 'Spherical'),
  alias = c('-', 'Squared-Exponential, or Stable Kernel', 'Power-Exponential', "Whittle-Mat\\'ern", '-'),
  fun = c('$\\exp\\left( -\\Delta \\right)$',
          '$\\exp\\left( -\\Delta^2 \\right)$',
          '$\\exp\\left( -\\Delta^p \\right)$',
          '$\\Delta^p K_p\\left( \\Delta \\right)$',
          '$1 - (3/2)\\Delta + (1/2)\\Delta^3$')
)
```

```{r cfun-table, eval = knitr::is_html_output()}
knitr::kable(cfun_df, format='html', col.names=cfun_colnames, caption=cfun_caption, escape=FALSE)
```

```{r cfun-table-latex, eval = knitr::is_latex_output()}
kableExtra::kable_styling(knitr::kable(cfun_df, format='latex', col.names=cfun_colnames, booktabs=TRUE, escape=FALSE, caption=cfun_caption), font_size=9)
```  



Parameters $\sigma^2$ and $\epsilon$ are the partial sill, and nugget variance, respectively. The partial sill, along with range parameters $\rho_x$ and $\rho_y$ (and any shape parameters), describe the variance of point values and the spatial profile of correlations between them. Any pair of correlograms $c_x$ and $c_y$ can be used, and their parameters may take on different values in the $x$ and $y$ directions.
Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cfun-table)', '\\@ref(tab:cfun-table-latex)'))`
lists the functional forms for $c$ currently implemented in \pkg{snapKrig}.

The nugget variance owes its name to historical applications in mining, where it originally accounted for fine-scale granular structure (nuggets) in the geological variables of interest. More generally, it can be interpreted as a measurement error term, where we assume that each observation $z_k$ is composed of a noise-free signal process plus an identically and independently distributed Gaussian random variable.

Readers are directed to the discussion in Chapter 3 of @cressie2015statistics for a more complete description these parameters and their various interpretations. In particular, it is helpful to understand their role in defining a graphical diagnostic known as the semi-variogram curve, some examples of which are found towards the end of the [Kriging Example] section. 

We call Equation \@ref(eq:covfun) a *Kronecker covariance* because when the observed data lie on a regular grid (and we assume they do), the matrix $V - \epsilon I$ takes the form of a Kronecker product. 

# Kriging example

\pkg{snapKrig} is primarily a kriging package, so we will demonstrate its core features in a point interpolation problem. Those familiar with the documentation for the \pkg{gstat} and \CRANpkg{sp} [@pebesma2005sp] packages will recognize the Meuse dataset, which appears in many of its R code examples [*eg.* @pebesma2022meuse].

The Meuse dataset is a survey of soil heavy metal concentrations in a river floodplain in the Netherlands, introduced by @burrough2015principles. We will interpolate the Meuse zinc concentration data while adjusting for a linear covariate, distance to river. 

The SGP is a model for a multivariate normal random variable, so users should always begin by considering whether their data fits this assumption closely enough. In our case, log-transforming the concentration data produces a response variable that more closely resembles a sample from the expected distribution. These log-zinc values been loaded already into a data-frame named `pts`. The following code uses `sk_snap` to snap them to a grid so that they can be used with \pkg{snapKrig}'s modelling functions.

```{r meuse-setup, include=TRUE, echo=TRUE}
# snap log zinc data to grid of specified resolution
g = sk_snap(pts, g=list(gres=c(y=50, x=50)))
summary(g)
```


```{r meuse-png, echo=FALSE, results='hide', fig.show='hold', out.width='50%', fig.dim=c(5,5), fig.cap='Zinc concentrations (in log parts per billion) from the Meuse dataset, a soil survey on the floodplain of the Meuse River (left). These data are snapped to a grid for use with snapKrig (right)', fig.alt='Two panes show two versions of the same soils data. The left pane shows a diagram of the winding Meuse river surrounded by about a hundred colored points. The points are coloured according to zinc levels at sample sites, and all are located in the vicinity of the river floodplain. In the right pane, a black-and-white grid of intersecting lines covers the same area. This grid is mostly empty (white), but a small number of cells are coloured - one for each point on the left pane.'}

# snap points to 50m x 50m grid
g = sk_snap(pts, g=list(gres=c(y=50, x=50)))

# plot source data using sf package, restoring default graphical parameters afterwards
plot(pts, pch=16, reset=FALSE, pal=.pal, key.pos=1, main='Meuse data', cex.main=0.8)
plot(st_geometry(pts), pch=1, add=TRUE)
plot(meuse_list[['river_poly']], col='lightblue', border=NA, add=TRUE)
plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot gridded version using the snapKrig package
plot(g, col_grid='lightgrey', reset=FALSE, zlab='log(ppb)', main='snapped at 50m resolution')
plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
```

Here we specified a resolution of 50 x 50 metres, and the function automatically selected a corresponding grid of dimensions 78 x 56 (rows x columns) inscribing the Meuse point sample locations (Figure \@ref(fig:meuse-png)). Alternatively, users can supply an existing grid (such as a DEM) with various options to match extents by cropping. 

At the selected resolution each Meuse data point is mapped to a unique grid point, and most positional errors are in the range of 10-25 metres distance which is good enough for this demonstration. In cases where two or more observations would snap to the same grid point, `sk_snap` omits all but the first observation, with a warning. In our experience, model-fitting results are insensitive to slight positional errors as long as the snapping process does not discard too many data points.    

## Ordinary kriging

The snapped data are now in the format expected by the \pkg{snapKrig}'s core modelling functions. A covariates data matrix `X` may optionally be supplied to account for linear trends, as we do later. First, by way of comparison, we will ignore covariates and fit a spatially constant trend.

`sk_fit` attempts to automatically find maximum likelihood estimators (MLEs) for the mean and covariance parameters by numerically optimizing the full joint likelihood for all observed points in `g`, using R's `stats::optim`.

```{r meuse-ok, include=TRUE, echo=TRUE}
# fit the covariance model and mean
fit_result_ok = sk_fit(g, quiet=TRUE)
```

The default covariance function implemented in `sk_fit` is the isotropic Gaussian, but a variety of alternatives are available
(see Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cfun-table)', '\\@ref(tab:cfun-table-latex)'))` and the [Model] section). Sensible defaults for initial values and bounds are set automatically based on the sample variance and grid dimensions.

To interpolate observed data, pass the covariance parameters (argument `pars`) and a grid containing the observed data (argument `g`) to `sk_cmean`. This populates all grid points in `g` with values from the kriging prediction equation (including at observed points). Variance is computed separately, but the calling syntax is the same apart from argument `out='v'`.

```{r meuse-ok-pred, echo=TRUE}
# compute conditional mean and variance 
g_ok = sk_cmean(g, pars=fit_result_ok)
g_ok_var = sk_cmean(g, pars=fit_result_ok, what='v', quiet=TRUE)
```


```{r meuse-ok-pred-png, results='hide', fig.show='hold', out.width='50%', fig.dim=c(5,5), fig.cap='Ordinary kriging prediction and variance heatmaps generated by snapKrig for the Meuse example. Predictions are generated for the entire grid, but they are masked in this plot to show detail in areas nearest the observed points.', fig.alt='Two heatmap images are shown, both masked to the same region as the previous figure. These heatmaps cover the convex hull of the observed zinc sample points, and within this region they show a range of smoothly varying colors.'}
# make a mask for high variance locations
is_var_high = g_ok_var[] > quantile(g_ok_var, 0.4, na.rm=TRUE)

# set masked pixels to NA on plot
g_ok_plot = g_ok 
g_ok_plot[is_var_high] = NA
g_ok_var_plot = g_ok_var
g_ok_var_plot[is_var_high] = NA

# plot predictor on log scale with river line 
sk_plot(g_ok_plot, axes=FALSE,
        zlab = 'log(ppb)',
        xlab = '',
        ylab = '',
        main = 'ordinary kriging predictor',
        col_box = 'grey',
        reset = FALSE,
        cex.main = 1.1)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot variance  
sk_plot(g_ok_var_plot, axes=FALSE,
        zlab = 'V(x,y)',
        xlab = '',
        ylab = '',
        main = 'ordinary kriging variance',
        col_box = 'grey',
        pal = 'Inferno',
        reset = FALSE,
        cex.main = 1.1)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)
```


Figure \@ref(fig:meuse-ok-pred-png) displays the output, masked to a neighbourhood of the observed data. Here we have requested predictions over the same grid that was used for fitting. However, users can request any output grid they like, by snapping the observed data to it and passing it to `sk_cmean` in argument `g`. 

The workflow up to this point has been ordinary kriging (OK), as we did not use any covariates to adjust for linear trends. We will do that now, by including distance to river as example predictors, and re-fitting the model to demonstrate universal kriging (UK).

## Universal kriging

To include covariates in a model fit, simply pass a matrix `X` in your call to `sk_fit` containing covariate values at the observed point locations. For prediction, the `X` argument in `sk_cmean` should include all of the output grid point locations.

Covariate data (rows) must be supplied in the same order as the grid data vector in `g`. In the code below we construct the full matrix `X` by using `sk_coords` to export the grid point locations to an `sf` points data-frame in the correct order, then we use `sf::st_distance` to compute distances to a line geometry representing the middle of the river.  

```{r meuse-uk-prep, echo=TRUE}
# measure distances for every point in the grid
river_dist = sf::st_distance(sk_coords(g, out='sf'), meuse_list[['river_line']])
river_dist = units::drop_units(river_dist)

# include distance covariate (and optionally its square root)
X = g
X[] = scale(river_dist)
#X[] = scale(cbind(river_dist, sqrt(river_dist)))
summary(X)
```

The UK workflow can now proceed like OK, except with `X` passed to `sk_fit` and `sk_cmean`.

```{r meuse-uk, echo=TRUE}
# fit the covariance model again with X
fit_result_uk = sk_fit(g, X=X, quiet=TRUE)

# compute conditional mean and variance
g_uk = sk_cmean(g, fit_result_uk, X)
g_uk_var = sk_cmean(g, fit_result_uk, X, what='v', quiet=TRUE)
```

These functions account for `X` by using the generalized least squares (GLS) equation for $\hat{\beta}$ to estimate the trend $X\beta$. Users can also pass their MLE covariance parameters to `sk_GLS` to get these estimates directly. We do this in the code below, then plot the result ($X\hat{\beta}$) in the left pane of Figure \@ref(fig:meuse-uk-pred-png). The middle and right panes show the resulting UK predictions and variance.

```{r meuse-lm, echo=TRUE}
# use GLS to estimate the spatially varying trend 
g_lm = sk_GLS(g, fit_result_uk, X=X)
```

```{r meuse-uk-pred-png, results='hide', fig.show='hold', out.width='33%', fig.dim=c(6,8), fig.cap='Universal kriging predictions and variance generated by snapKrig for the Meuse example. The response variable (log zinc concentration) is de-trended using a linear predictor (left) based on distance to river, resulting in more detail in kriging predictions (middle) and variance (right)', fig.alt='Three heatmap images are shown, all masked to the same region as the previous two figures (centered over the winding Meuse River). In the left pane, colours becoming brighter near the river; In middle pane, colors vary smoothly to roughly match the colours of the point data in the previous figure. In the right pane, colors vary smoothly in a way that matches the density of the points in the previous figure.'}

# mask same pixels as previous plot
g_lm_plot = g_lm
g_uk_plot = g_uk
g_uk_var_plot = g_uk_var
g_lm_plot[is_var_high] = NA
g_uk_plot[is_var_high] = NA
g_uk_var_plot[is_var_high] = NA

# plot linear predictor on log scale with river line 
sk_plot(g_lm_plot, axes=FALSE,
        zlab = 'log(ppb)',
        xlab = '', ylab = '',
        main = 'covariates',
        col_box = 'grey',
        cex.main = 2.1,
        cex.z = 1.5,
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot kriging predictor on log scale with river line 
sk_plot(g_uk_plot, axes=FALSE,
        zlab = 'log(ppb)',
        xlab = '', ylab = '',
        main = 'universal kriging predictor',
        col_box = 'grey',
        cex.main = 2.1,
        cex.z = 1.5,
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot kriging variance on log scale with river line 
sk_plot(g_uk_var_plot, axes=FALSE,
        zlab = 'V(x,y)',
        xlab = '', ylab = '',
        main = 'universal kriging variance',
        col_box = 'grey',
        pal = 'Inferno',
        cex.main = 2.1,
        cex.z = 1.5,
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

```


## Diagnostics

Kriging theory guarantees that `sk_cmean` will return the best linear unbiased predictor (in the sense of minimizing variance) provided that the covariance model is a good fit to the data. To find the best fitting model for their data, users are encouraged to seek out informative covariates to include in `X` and to check model fitting diagnostics for problems that would impact predictions.

\pkg{snapKrig} offers two visual diagnostics for covariance parameters. The first, `sk_plot_pars`, draws a heatmap of covariances between a given grid-point and its surroundings. This indicates the degree of smoothing to be expected in predictions, and the direction of anisotropy (if any).

The second diagnostic, `sk_plot_semi`, plots the semi-variogram function, ${\gamma}$, for a given covariance model. It is common practice in geostatistics to compare candidates for ${\gamma}$ against a point cloud of sample semi-variances, $\hat{\gamma}$, which are estimated directly from the data (*ie.* independently of the covariance model). If the estimator $\hat{\gamma}$ is constructed properly, its distribution should be centered around the true $\gamma$.

Sample semi-variograms are computationally fast and easy to interpret, but it is not always clear which form of $\hat{\gamma}$ to use, or how much to trust it. Errors in trend estimation cause $\hat{\gamma}$ to overestimate $\gamma$ [@cressie2015statistics], and even when $\hat{\gamma}$ is unbiased it can vary substantially from sample to sample, causing problems of reproducibility. For these reasons \pkg{snapKrig} does not provide any methods for fitting ${\gamma}$ by least squares. However it implements common methods for visualizing $\hat{\gamma}$ in the functions `sk_plot_semi` and `sk_sample_vg`. Users are encouraged to experiment with simulated data for a known $\gamma$ (*eg.* using `sk_sim`) to get a sense of the variability to expect in sample distributions of $\hat{\gamma}$.

As a final diagnostic for MLE, users should check that the initial values and bounds used in model-fitting are reasonable. These are reported in the output of `sk_fit` (attribute `'bds'`) and can be adjusted manually in the function call. Be on the lookout for common optimization issues, such as parameters converging to a bound, or not moving from their initial values. Both can indicate problems of model-misspecification, or simply a poor choice of initial values. 


```{r meuse-ok-vg, results='hide'}
# compute sample semivariogram for the OK model
vg_ok = sk_sample_vg(g)

# recompute variogram with trend removed
vg_UK = sk_sample_vg(g - g_lm)
```

```{r meuse-vg-png, fig.show='hold', out.width='50%', fig.dim=c(4, 3), fig.pos='!htb', fig.cap='Fitted covariance models from kriging on the Meuse dataset, visualized in two ways: On the left, estimated semi-variogram values (circles) are plotted next to fitted model values (blue curves). On the right, a heatmap displays covariances with respect to the central point. The top two plots show the fitted OK model. The bottom two plots show the UK model, where removing a linear trend from the response data has resulted in higher variance and a smaller range.', fig.alt='A four panel plot arranged in a square: the top-left pane is a scatterplot of points in the shape of a hill, with a smooth blue curve approximately following the pattern of points. The top-right pane shows a heatmap resembling a blurry photo of a ball. The bottom two panes are similar, but the point pattern in the scatterplot (bottom-left) traces out a much shallower hill, and in the blurry heatmap image (bottom-right), the ball is much smaller.'}

# plot the sample semi-variogram with theoretical curve in blue for OK model fit
par(mar=c(5.1, 5.1, 4.1, 2.1))
sk_plot_semi(vg_ok, fit_result_ok,
             main = 'OK model semi-variogram', 
             lwd = 2,
             d_max = 2 * fit_result_ok$y$kp, 
             alpha_bin_b = 0.2, 
             alpha_bin = 0.2,
             alpha_model = 0.3)

box('outer', col='lightgrey')
par(.par_default)

# plot correlation heatmap for OK model fit
g_plot = modifyList(g, list(gdim=rep(min(g[['gdim']]), 2), gyx=NULL, gval=NULL))
sk_plot_pars(fit_result_ok, g_plot, ij=T, main='', col_box = 'grey')

# plot the sample semi-variogram with theoretical curve in blue
par(mar=c(5.1, 5.1, 4.1, 2.1))
sk_plot_semi(vg_UK, fit_result_uk, 
             main = 'UK model semi-variogram', 
             lwd = 2,
             d_max = 2 * fit_result_uk$y$kp, 
             alpha_bin_b = 0.2, 
             alpha_bin = 0.2,
             alpha_model = 0.3)

box('outer', col='lightgrey')
par(.par_default)

# plot correlation heatmap
g_plot = modifyList(g, list(gdim=rep(min(g[['gdim']]), 2), gyx=NULL, gval=NULL))
sk_plot_pars(fit_result_uk, g_plot, ij=T, main='', col_box = 'grey')


```

Diagnostics from the OK and UK workflows are plotted together in Figure \@ref(fig:meuse-vg-png) for comparison. Accounting for distance to the river has a dramatic effect on the fitted covariance model. In the UK model the partial sill is higher, indicating more spatial variability, but the estimated measurement error and effective range are much smaller. This leads to more spatial detail and less smoothing in the kriging prediction and variance (Figure \@ref(fig:meuse-uk-pred-png)) compared with OK (Figure \@ref(fig:meuse-ok-pred-png)).


## Anistropic models

Another way to refine model fit is using anisotropic covariance functions. The term *isotropic* refers to radial symmetry in $C$, where equal distance implies equal covariance, and *anisotropic* refers to its absence. By default `sk_fit` uses an isotropic Gaussian model, but \pkg{snapKrig} supports covariance functions of both kinds.

Users can relax the isotropy assumption by calling `sk_fit` with argument `iso=FALSE`. This allows the component range parameters, $\rho_x$ and $\rho_y$, to vary independently.

```{r meuse-uk-aniso, echo=TRUE}
# fit Gaussian covariance with geometric anisotropy
fit_result_uk_gau = sk_fit(g, pars='gau', X=X, iso=FALSE, quiet=TRUE)
```

The argument `pars` specifies the covariance model. For example, `pars='gau'` above is shorthand for `pars=c(y='gau', x='gau')`, which specifies that $c_x$ and $c_y$ should both be Gaussian correlograms. This default can be changed to any of the names in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cfun-table)', '\\@ref(tab:cfun-table-latex)'))`, or any pair of them. The following code fits four additional anisotropic models using the same shorthand.

```{r meuse-uk-aniso2, echo=TRUE}
# fit four other covariance functions with anisotropy
fit_result_uk_sph = sk_fit(g, pars='sph', X=X, iso=FALSE, quiet=TRUE)
fit_result_uk_exp = sk_fit(g, pars='exp', X=X, iso=FALSE, quiet=TRUE)
fit_result_uk_mat = sk_fit(g, pars='mat', X=X, iso=FALSE, quiet=TRUE)
fit_result_uk_gxp = sk_fit(g, pars='gxp', X=X, iso=FALSE, quiet=TRUE)
```

The Matérn and gamma-exponential correlograms (`'mat'` and `'gxp'`) include an additional shape parameter $p$ that controls the degree of kurtosis. As with the range, `iso=FALSE` allows this parameter to vary independently in the x and y directions. The fitted parameters from all five anisotropic models are illustrated as heatmaps in Figure \@ref(fig:meuse-alt-fit). 


```{r meuse-alt-fit, fig.show='hold', out.width='19%', fig.dim=c(5,5), fig.pos='!htb', fig.cap='Five examples of anisotropic covariance structures fitted to the Meuse data. As in the previous figure, darker pixels indicate stronger correlation with the central point. From left to right, these are the Kronecker covariances formed by setting both $c_x$ and $c_y$ equal to "gau", "mat", "gxp", "sph", and "exp" models, respectively.', fig.alt='A sequence of five heatmap panes are displayed side by side. Each one shows an image resembling a blurry photo of a small object. In the first three panes, this ball is egg-shaped and the heatmaps are nearly identical. In the last two panes (and particularly, in the far right pane), the ball is shaped more like a diamond, with corners oriented up-down and left-right.'}

# compile all fitted parameters
pars_list = list(uk_gau = fit_result_uk_gau,
                 uk_mat = fit_result_uk_mat,
                 uk_gxp = fit_result_uk_gxp,
                 uk_sph = fit_result_uk_sph,
                 uk_exp = fit_result_uk_exp)

# make a list of titles
title_list = c('Gaussian', 'Matérn', 'Gamma-Exponential', 'Spherical', 'Exponential')

# plot everything in minimal style
g_plot = modifyList(g, list(gdim=rep(min(g[['gdim']]), 2), gyx=NULL, gval=NULL))
cex_main_kernels = 2.5
plot_result = Map(\(p, m) {
  
  sk_plot_pars(p, g_plot,
             minimal = TRUE,
             col_box = 'grey',
             main = m,
             cex.main = cex_main_kernels,
             xlab = '', ylab = '',
             pal = 'Inferno')

  }, p=pars_list, m=title_list)

par(.par_default)

```


The Gaussian is unique in being the only Kronecker covariance capable of isotropy. For example, `pars='mat'` does *not* specify the common 2-d Matérn function, which is isotropic but not a Kronecker covariance. Rather it specifies a product of 1-d Matérn correlograms, which is always anisotropic. This model can closely approximate the 2-d Matérn for certain parameters [@koch2020unifying], and it converges to the Gaussian in the limit of large shape $p\to\infty$.

This large-$p$ behavior is clear in Figure \@ref(fig:meuse-alt-fit), where the first three fitted models have all converged on the same Gaussian covariance structure. The MLEs for $p$ in the Matérn product (second panel) have converged to their upper bounds, producing a close approximation to the Gaussian (first panel). Similarly, the MLEs for the gamma-exponential both converged to $\hat{p}=2$, which is equivalent to the Gaussian.

Notice that that the direction of anisotropy is always orthogonal to the coordinate axes. This cannot be changed. However, different orientations could be modeled by estimating the direction of anisotropy in a preliminary analysis [*eg.* by the method of @koch2020computationally] then rotating the observed point locations by this angle prior to snapping. 

Anisotropic covariance functions impart complexity in the form of new parameters, but also flexibility. This can improve a model by more closely aligning it with reality, or it can worsen it by enabling over-fitting, which can lead to underestimates of uncertainty and bias in predictions.

```{r meuse-alt-pred, results='hide'}

# compile all fitted parameters
pars_list = list(uk_gau = fit_result_uk_gau,
                 uk_mat = fit_result_uk_mat,
                 uk_gxp = fit_result_uk_gxp,
                 uk_sph = fit_result_uk_sph,
                 uk_exp = fit_result_uk_exp)
        
# collect all model fitting results into a list
all_results = c(list(ok=fit_result_ok), pars_list)

# check likelihood, AIC and BIC for all models
all_uk = sapply(all_results, \(p) sk_LL(p, g, X, out='m'))

# compute conditional kriging mean and variance for three of the models 
g_uk_mat = sk_cmean(g, fit_result_uk_mat, X)
g_uk_sph = sk_cmean(g, fit_result_uk_sph, X)
g_uk_exp = sk_cmean(g, fit_result_uk_exp, X)
g_uk_mat_var = sk_cmean(g, fit_result_uk_mat, X, what='v')
g_uk_sph_var = sk_cmean(g, fit_result_uk_sph, X, what='v')
g_uk_exp_var = sk_cmean(g, fit_result_uk_exp, X, what='v')

# check fitted vs predicted
# plot(g[!is.na(g)], g_uk_sph[!is.na(g)])
# abline(0,1)
# plot(g[!is.na(g)], g_uk_exp[!is.na(g)])
# abline(0,1)
# plot(g[!is.na(g)], g_uk_mat[!is.na(g)])
# abline(0,1)
# plot(g[!is.na(g)], g_uk[!is.na(g)])
# abline(0,1)
# plot(g[!is.na(g)], g_ok[!is.na(g)])
# abline(0,1)

```

```{r meuse-alt-pred-png, results='hide', fig.show='hold', out.width='33%', fig.dim=c(5,6), fig.pos='!bht', fig.cap='Universal kriging predictions for the Meuse example using three anisotropic Kronecker covariance models, based on the Gaussian, exponential, and spherical correlograms.', fig.alt='A set of three heatmaps are shown side-by-side.These depicting the same aerial view of the Meuse river floodplain as shown in the the first two figures of this paper. Each pane shows a smoothly varying field of colours roughly matching to the colours of the sample points displayed in Figure 1. The three images are very similar, but the first pane is noticeably smoother than the others'}

# mask same pixels as previous plot
g_uk_gau_plot = g_uk_mat 
g_uk_exp_plot = g_uk_exp 
g_uk_sph_plot = g_uk_sph 
g_uk_gau_plot[is_var_high] = NA
g_uk_exp_plot[is_var_high] = NA
g_uk_sph_plot[is_var_high] = NA

zlim = range(sapply(list(g_uk_gau_plot, g_uk_exp_plot, g_uk_sph_plot), range, na.rm=TRUE))

# plot kriging predictor on log scale with river line 
sk_plot(g_uk_gau_plot, axes=FALSE, zlim=zlim,
        zlab = 'log(ppb)\n',
        xlab = '', ylab = '',
        main = 'Gaussian',
        cex.main = 1.6,
        cex.z = 1.5,
        col_box='grey',
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot kriging variance on log scale with river line 
sk_plot(g_uk_exp_plot, axes=FALSE, zlim=zlim,
        zlab = 'log(ppb)\n',
        xlab = '', ylab = '',
        main = 'Exponential',
        cex.main = 1.6,
        cex.z = 1.5,
        col_box = 'grey',
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

# plot kriging variance on log scale with river line 
sk_plot(g_uk_sph_plot, axes=FALSE, zlim=zlim,
        zlab = 'log(ppb)\n',
        xlab = '', ylab = '',
        main = 'Spherical',
        cex.main = 1.6,
        cex.z = 1.5,
        col_box = 'grey',
        reset = FALSE)

plot(meuse_list[['river_line']], col='black', lwd=1, add=TRUE)
par(.par_default)

```

Figure \@ref(fig:meuse-alt-pred-png) shows predictions from three of the anisotropic UK models fitted in the previous section. These have more spatial complexity than the isotropic UK model fitted earlier on. Will this improve predictions at unseen locations, or are we over-fitting?


## Model comparisons

The easiest way to compare model fit is using information criteria such as AIC or BIC. These balance negative log-likelihood with a penalty for complexity (lower is better). \pkg{snapKrig}'s log-likelihood function, `sk_LL`, can be called with argument `what='m'` to return these statistics. They are reported in Table
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cv-table)', '\\@ref(tab:cv-table-latex)'))` for all of the models fitted so far.

Both AIC and BIC tell the same story here. Introducing a covariate (UK) improves the fit compared with OK, but allowing anisotropy does not. The preferred covariance model is the default isotropic Gaussian (Figure \@ref(fig:meuse-uk-pred-png)), with the anisotropic Gaussian (Figure \@ref(fig:meuse-alt-pred-png), left) a close second.

```{r meuse-cv-prep}

# copy fitted model parameters
anisotropy_pars = list(uk_gau = fit_result_uk_gau,
                          uk_mat = fit_result_uk_mat,
                          uk_gxp = fit_result_uk_gxp,
                          uk_sph = fit_result_uk_sph,
                          uk_exp = fit_result_uk_exp)

# collect all model fitting results into a list
all_pars = c(list(ok=fit_result_ok, uk=fit_result_uk), anisotropy_pars)

# check likelihood, AIC and BIC for all universal kriging models
all_uk_results = data.frame(lapply(all_pars[-1], \(p) unlist(sk_LL(p, g, X, out='m'))))

# combine with results on ok model (omit X from this call)
ok_results = data.frame(ok=unlist(sk_LL(fit_result_ok, g, X=NA, out='m')))

all_results = cbind(ok_results, all_uk_results)
all_IC = data.frame(t(all_results[c('AIC', 'BIC'),])) 

# load pre-computed CV results
cv_results_all = read.csv(path_cv_result, row.names=NULL)

# create a data frame with everything relevant
table_df = cbind(all_IC, cv_results_all[c('isotropic', 'covariates', 'parameters', 'rMSPE', 'rMSPEb')])
table_df[['$c_x$']] = c('gau', 'gau', 'gau', 'mat', 'gxp', 'sph', 'exp')
table_df[['$c_y$']] = table_df[['$c_x$']]
table_df = table_df[c('covariates', 'isotropic', '$c_y$', '$c_x$', 'parameters', 'AIC', 'BIC', 'rMSPE', 'rMSPEb')]
rownames(table_df) = NULL

# formatting
table_df[['covariates']] = c('no', 'yes')[1+as.logical(table_df[['covariates']])]
table_df[['isotropic']] = c('no', 'yes')[1+as.logical(table_df[['isotropic']])]
table_df[['AIC']] = round(table_df[['AIC']], 1)
table_df[['BIC']] = round(table_df[['BIC']], 1)
table_df[['rMSPE']] = round(table_df[['rMSPE']], 3)
table_df[['rMSPEb']] = round(table_df[['rMSPEb']], 2)

# define caption text here
caption_cv = 'Estimates of the root mean squared prediction error on the Meuse dataset for log zinc (rMSPE) and its back-transformed values (rMSPEb) in a 25 X 5-fold cross-validation (CV) experiment. Results are reported for all seven of the kriging models presented in this paper, along with the AIC and BIC scores for models fitted to all observed data.'
```

```{r cv-table, eval = knitr::is_html_output()}
knitr::kable(table_df, format='html', caption=caption_cv)
```

```{r cv-table-latex, eval = knitr::is_latex_output()}
kableExtra::kable_styling(knitr::kable(table_df, format='latex', booktabs=TRUE, linesep='', caption=caption_cv, escape=FALSE), font_size=9)
```  


Cross-validation (CV) is another useful tool for model selection, particularly when the goal is prediction. In CV, we withhold a subset of the data (a hold-out set, or fold) during model fitting, and then come back to it later as a reference for checking the quality of predictions. The hold-out error indicates what to expect when predicting at an unseen locations. If a model is over-fitted, it will predict well on the training locations, but poorly on the hold-out locations. 

We performed a 5-fold cross-validation experiment by dividing the 155 observed Meuse points randomly into five hold-out sets, each of size n=31. For each fold, we fitted all seven models to the complementary subset of the data (size n=124), then computed the mean squared prediction error on the log-zinc scale at hold-out locations (MSPE). We also computed its back-transformed version, MSPEb (see the [Back-transforming] section below).

As these statistics were sensitive to the choice of partition, we repeated the process 25 times and averaged the results. They are reported in Table
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cv-table)', '\\@ref(tab:cv-table-latex)'))`. 
We see an obvious improvement in CV prediction error after adding the covariate, consistent with the AIC/BIC results. However, the addition of anisotropy improved all of the CV scores, with the exponential function performing best overall, followed closely by the spherical function. Predictions from these two models are plotted in the middle and right panes of Figure \@ref(fig:meuse-alt-pred-png).

Lower CV scores suggest better predictive ability in terms of MSPE, but they are not necessarily evidence of a better model fit. The difference is subtle but important. Kriging theory hinges on the assumption of a well-fitted model, so if the goal is parameter inference, or if reliable variance estimates are important, then users should prioritize model fit and seek to minimize AIC/BIC. If the priority is to generate predictions with minimal MSPE, users may instead wish to minimize CV error.

## Back-transforming

When predictions are required on the original scale (ppb, instead of its logarithm), users may think to simply take the exponential of the kriging predictions vector `z_uk`. However this leads to underestimates of the (random) variable $\exp(z)$ (by Jensen's inequality), so we recommend adding one half the kriging variance to `z_uk` before exponentiating [@cressie2015statistics], as we did before computing rMSPEb in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:cv-table)', '\\@ref(tab:cv-table-latex)'))`. Note that this particular bias-adjustment is specific to the log-transformation.

# Computations

This last section looks at the computational bottlenecks that can make kriging slow, and that techniques that \pkg{snapKrig} uses to circumvent them. The core idea is to rewrite the matrix $V - \epsilon I$ as a Kronecker product (see the [Model] section), so that operations involving sub-matrices of $V$ can be simplified.

Two sub-matrices are particularly important. The first is the $n_o \times n_o$ covariance $V_o$, for the $n_o$ observed points. $V_o$ has to be generated (its entries calculated), then factorized, so that products of the form $V_o^{-1} z$ can be calculated. This happens in many routine spatial statistical operations, including likelihood, GLS, kriging equations, and conditional simulation, and it is the reason they become computationally intensive for large $n_o$.  

The second important sub-matrix is the $n_p \times n_o$ cross-covariance $V_p$, between the observed locations and the $n_p$ unseen prediction locations. $n_p$ can be very large, making products with $V_p$ expensive both in terms of computation time and memory use. Such products appear in both the kriging prediction and variance equations.

\pkg{snapKrig} generates the entries of $V_o$ and $V_p$ directly from the Kronecker product factorization of $V - \epsilon I$. This speeds construction time for $V_o$ and $V_p$, and, more importantly, it enables the package to multiply a vector by $V_p$ without having to explicitly build $V_p$ in computer memory. 


## Benchmark comparisons

To illustrate the speed of \pkg{snapKrig} we timed computations for the Meuse OK exercise presented earlier in the [Kriging Example] section, repeating it several times over a range of output resolutions (varying $n_p$) and recording median evaluation times in five repetitions using \CRANpkg{microbenchmark} [@mersmann2021micro]. To limit the total length of the experiment, all function calls were halted (timed-out) at around two minutes. 

For comparison we tested three popular R packages mentioned in the introduction, \pkg{gstat}, \pkg{geoR}, \pkg{fields}, all using the same isotropic Gaussian model fitted with default settings. To get a range of observed sample sizes ($n_o$) values we repeated this process with two other point datasets, one on ozone concentration, and one on forest density. 

The ozone data are included as the object `ChicagoO3` in the \pkg{fields} package [@nychka2021fields] and appear frequently in its example code. They are Environmental Protection Agency recordings of average ozone concentration in the air at $n_o=20$ Chicago-area stations in 1987.

The forest data come from Canada-wide estimates of forest characteristics by @beaudoin2018tracking. Estimates of areal tree cover (%) for the province of British Columbia (BC) are reproduced in \CRANpkg{rasterbc} [@koch2022rasterbc] as raster tiles, each containing around one million data points. We created a much smaller example dataset, which we call "treed", by sampling one thousand points at random from one of these tiles, located in Central BC (Figure \@ref(fig:treed-png), left).


```{r treed-png, results='hide', fig.dim=c(4.5,4), fig.show='hold', out.width='33%', fig.pos='htb', fig.cap='A 1021 x 1349 raster on forest density in central BC, Canada (left) was up-scaled to produce a much coarser resolution version with dimensions 32 x 43 (middle). snapKrig is optimized for downscaling rasters like these. Model fitting and prediction of over a million points at original resolution (right), with elevation as a predictor, took less than three seconds.', fig.alt='Three panels are shown side-by-side, each one showing pine forest cover over a common area, as it would appear looking down from an aircraft. The images are dominated by green, corresponding to dense forest, with exceptions like high alpine and river valleys in depicted in white, where no pine grow. The left panel is a high-resolution image of the source data. The middle panel shows the same image at very coarse resolution. The right panel has the same resolution as the left panel, and contains much of the same information, but the image is more blurry.'}

# load the treed raster data and a matching DEM
g_treed = sk(rast(path_treed))
g_dem = sk(rast(path_dem))

# make a training set at much coarser resolution
g_train = sk_rescale(g_treed, up=32)
g_train_dem = sk_rescale(g_dem, up=32)

# snap training points to original grid (NAs everywhere else)
g_out = sk_snap(sk_coords(g_train, out='sf'), g_treed)

# use elevation and (optionally its square root) as covariate
X = g_dem
#X[] = scale(cbind(X[], sqrt(X)[]))
X[] = scale(X[])

# smaller version for training
X_train = g_train_dem
#X_train[] = scale(cbind(X_train[], sqrt(X_train)[]))
X_train[] = scale(X_train[])

# fit the model
start_fit = Sys.time()
  treed_fit_result = sk_fit(g_train, X=X_train, iso=FALSE, pars='gau', quiet=TRUE)
end_fit = Sys.time()

# make predictions
start_pred = Sys.time()
  g_treed_pred = sk_cmean(g_out, X=X, pars=treed_fit_result)
end_pred = Sys.time()

# report both times
print(end_fit-start_fit)
print(end_pred-start_pred)

# find a common range for colorbar
zlim = range(c(g_treed_pred[], g_train[], g_treed[]), na.rm=TRUE)
pal_nm = 'YlGn'
pal = function(n) { hcl.colors(n, pal_nm, rev=TRUE) }

# draw the three plot panes
sk_plot(g_treed, zlim=zlim,
        main = 'treed data (original scale)',
        col_box = 'grey',
        minimal = TRUE,
        pal = pal_nm,
        rev = TRUE,
        cex.main = 1.4,
        col_box = 'black')

sk_plot(g_train, zlim=zlim,
        main = 'treed data (up-scaled)',
        col_box  =  'grey',
        minimal = TRUE,
        col_grid = 'white',
        pal = pal_nm,
        cex.main = 1.4,
        rev = TRUE)

sk_plot(g_treed_pred, zlim=zlim,
        zlab = 'density',
        main = 'kriging predictor', 
        col_box  =  'grey',
        minimal = TRUE,
        pal = pal_nm,
        cex.main = 1.4,
        rev = TRUE)

```

We also created a sequence of eight up-scaled (coarser resolution) versions of this raster, to serve as examples of raster data inputs. We name these datasets according to their sample sizes, with the suffix indicating $n_o$ (treed_88, treed_352, and so on). Figure \@ref(fig:treed-png) (middle) plots the third in the sequence, with $n_o=1376$. These raster examples are special because they are complete, having no `NA` values. The three irregularly sampled point sets (ozone, Meuse, and treed) on the other hand are incomplete, since their empty un-sampled spaces are filled with `NA`s. 

## Complete data

This distinction between complete and incomplete is important because both $V - \epsilon I$ and $V_o - \epsilon I$ have Kronecker product factorizations in the complete case, making $V_o$ much easier to deal with. The result is big performance improvements in \pkg{snapKrig} on large $n_o$ problems, particularly in evaluations of the likelihood function.

```{r bench-results}
# load the benchmarking results
results_df = read.csv(path_bench_result, row.names=NULL)
results_names = unique(results_df[['name']])
results_df[['is_complete']] = c('no', 'yes')[1 + as.integer(results_df[['complete']])]

```

```{r bench-fit-png, fig.dim=c(5,2.5), fig.pos='!htb', fig.cap='Time required to evaluate the likelihood function for example data sets with a range of sample sizes ($n_o$). Cases where the observed data form a complete regular grid are indicated by dashed lines, whereas incomplete cases are indicated with solid lines. The likelihood function in snapKrig optimized for low memory use and fast computation in the complete case.', fig.alt='A line chart shows several different coloured lines (representing different packages) increasing at an exponential rate. Each one roughly follows the same trajectory except the blue dotted line (snapKrig, with complete data), which has a much shallower growth rate.'}

# plot time to fit the model against number of observed points
ggplot(subset(results_df, pkg != 'gstat')) +
  aes(x=n_in, y=teval_lik, color=pkg, lty=is_complete) +
  geom_point(size=1, pch=1) +
  geom_line(lwd=0.5) +
  scale_linetype_manual(values=c(no='solid', yes='11')) +
  xlab('number of observed points') +
  ylab('likelihood function evaluation time (seconds)') +
  labs(color='R package',
       lty='complete grid') +
  scale_x_log10(
    breaks = scales::trans_breaks('log10', function(x) 10^x),
    labels = scales::trans_format('log10', scales::math_format(10^.x))
  ) +
  scale_y_log10(
    breaks = scales::trans_breaks('log10', function(x) 10^x),
    labels = scales::trans_format('log10', scales::math_format(10^.x))
  ) +
  theme_bw() +
  theme(text=element_text(size=8),
        strip.text.x=element_text(face='bold'),
        strip.text.y=element_text(face='bold'))
```

This improvement is illustrated in Figure \@ref(fig:bench-fit-png), which plots likelihood evaluation time against $n_o$. We excluded \pkg{gstat} from this comparison because it uses variograms (not MLE) to parametrize covariance. The other packages all follow a similar trajectory of rising computation times, with the exception of \pkg{snapKrig} in the complete data case (dotted lines). Speedy likelihood function evaluations by \pkg{snapKrig} led to dramatically faster fitting times for complete data with large $n_o$. 

In a log-log plot like Figure \@ref(fig:bench-fit-png), a linear trend with slope $s$ reflects a complexity power law with exponent $s$. What is noteworthy here is in the large-$n_o$ behavior, which shows a similar $s$ for all packages except \pkg{snapKrig} on complete examples, where it is faster than the rest by a factor that grows *exponentially* with $n_o$. These differences reflects the rate-limiting operation of factoring $V_o$. In \pkg{snapKrig} its algorithmic complexity is $O( n_o^3 )$ for incomplete data, and closer to $O(n_o^{3/2})$ for complete data.

\pkg{snapKrig}'s performance edge with complete data is not limited to kriging-related problems. We can expect similar improvements in computation time for any application of likelihood, GLS, or simulations from SGPs. Note however that the completeness requirement is strict - any number of `NA`s in the data grid means the grid is incomplete


## Prediction and variance

We timed evaluations of the kriging prediction and variance equations for ten prediction grids, ranging in size from $8 \times 11$ ($n_p=88$) to $4084 \times 5396$ ($n_p>$ 22 million). For \pkg{snapKrig} and \pkg{fields}, we timed kriging prediction and variance separately. In \pkg{gstat} and \pkg{geoR}, both are returned from a single function call, so we only recorded the combined time.

```{r bench-pred-png, fig.dim=c(6,4), fig.pos='[!tb]', escape=FALSE, fig.cap='A comparison of computational performance in kriging as a function of the number of points predicted, $n_p$. Dashed lines indicate prediction time on its own, and solid lines indicate time to compute both predictions and variance. Panels separate results from six examples. Prediction with snapKrig is much faster on large-$n_p$ problems than the tested alternatives. This improvement grows with number of observed points $n_o$, and is most pronounced on complete grid problems (bottom row).', fig.alt='A set of six panels are shown, one for each of the (differently sized) datasets. In each panel a set of coloured lines run from the lower left to the upper right, showing computation time increasing with prediction set size.'}
# names of the two groups of example datasets (pts = incomplete, rast = complete)
pts_names = c('ozone', 'meuse', 'treed')
rast_names = paste0('treed_', c(88, 352, 1376))

# reorder data to reflect the panel order we want in the plot
results_plot_df = results_df[ results_df[['name']] %in% c(pts_names, rast_names),]
results_plot_df = results_plot_df[order( match(results_plot_df[['name']], c(pts_names, rast_names)) ),]

# create titles with sample size and order by input size
results_plot_df[['n_plot']] = paste0('(n_o=', results_plot_df[['n_in']], ')')
results_plot_df[['name_plot']] = apply(results_plot_df[, c('name', 'n_plot')], 1, paste, collapse=' ')
results_plot_df[['name_plot']] = factor(results_plot_df[['name_plot']], levels=unique(results_plot_df[['name_plot']]))

# make a plotting data frame with single column for both times
n_case = nrow(results_plot_df)
results_plot_df = results_plot_df[rep(seq(n_case), 2),]
results_plot_df[['with_var']] = rep(c('yes', 'no'), each=n_case)
results_plot_df[['teval']] = results_plot_df[['teval_pred']]
results_plot_df[['teval']][seq(n_case)] = results_plot_df[['teval_both']][seq(n_case)]

# make the plot
ggplot(results_plot_df) +
  aes(x=n_out, y=teval, color=pkg, lty=with_var) +
  geom_point(size=1, pch=1) +
  geom_line(lwd=0.5) +
  scale_linetype_manual(values = c(no='11', yes='solid')) +
  xlab('number of points predicted') + 
  ylab('prediction time (seconds)') +
  labs(color='R package',
       lty='with variance') +
  scale_x_log10(
   breaks = scales::trans_breaks('log10', function(x) 10^x),
   labels = scales::trans_format('log10', scales::math_format(10^.x))
  ) +
  scale_y_log10(
   breaks = scales::trans_breaks('log10', function(x) 10^x),
   labels = scales::trans_format('log10', scales::math_format(10^.x))
  ) +
  facet_wrap(vars(name_plot)) +
  #scale_color_viridis(discrete=TRUE, option='turbo') +
  theme_bw() +
  theme(text=element_text(size=8),
        strip.text.x=element_text(face='bold'),
        strip.text.y=element_text(face='bold'))

```

The times are plotted in Figure \@ref(fig:bench-pred-png) as a function of prediction grid size, $n_p$. The top three panels show results from the incomplete data examples, and the bottom three show the smallest of the complete (raster) examples.

Notice the relative speed of prediction (dashed lines) compared to variance and prediction (solid lines). This is because in the variance equation $V_p$ is multiplied by an $n_o \times n_o$ matrix, whereas in prediction it is multiplied by a length-$n_o$ vector (lowering complexity by a factor of $n_o$). This makes \pkg{fields}, and \pkg{snapKrig} stand in out the large $n_p$ results for having fast prediction-only methods.

Even with incomplete data, \pkg{snapKrig} was the fastest on all but the smallest $n_p$ and $n_o$ examples. As with likelihood, this relative speed-up increases with $n_o$. By $n_o=1000$, prediction for large $n_p$ is 2+ orders of magnitude faster than the next fastest method. Variance was also faster with large $n_o$ on the incomplete examples, but to a lesser extent.

Computation times improved even further with complete data, particularly with respect to variance. Combined times for \pkg{snapKrig} were faster than any other package by 1-2 orders of magnitude in all but the smallest $n_p$ examples. The addition of covariates and anisotropy introduces little additional computational complexity. See for example Figure \@ref(fig:treed-png) where we demonstrate UK with anisotropic covariance for the treed_1376 dataset.

\pkg{snapKrig} computes the variance in an unusual way for both complete and incomplete problems. Instead of multiplying $V_p$ by the $n_o \times n_o$ matrix $V_o^{-1}$, `sk_cmean` multiplies it by the eigen-vectors of $V_o$ in a (length-$n_o$) loop, combining results as it goes. This is slower than matrix multiplication but it greatly reduces computer memory demands on problems with $n_o$ and $n_p$ both large, as we avoid ever having to write $V_p$ or the product $V_p V_o^{-1}$ entirely in memory during kriging.



# Summary

By specializing for Kronecker covariance, \pkg{snapKrig} raises the upper limits for practical sample sizes and resolutions in SGP-based analysis, leading to a more interactive and less frustrating user experience for analysts.

Kronecker covariances are common enough in geostatistics. We see them everywhere in the form of Gaussian covariance functions, and separable space-time models. Nevertheless, we are unaware of any other CRAN R package for geostatistics that factors the *spatial* covariance $V$ into a Kronecker product, as we do in \pkg{snapKrig}.

This factorization leads to huge computational advantages in evaluating important modelling equations such as likelihood (Figure \@ref(fig:bench-fit-png)) in kriging (Figure \@ref(fig:bench-pred-png)), with speed-ups of 1-3 orders of magnitude on problems with hundreds to thousands of observed points.

We showed, for example, how this makes it straightforward to downscale a small raster onto a grid of several million points in seconds (rather than minutes) using an MLE-based OK or UK model. \pkg{snapKrig} is naturally suited down-scaling with complete data, and its computational speed makes it a practical yet statistically principled alternative to other fast interpolation methods like inverse distance weighting and Voronoi tiling, which, despite their bias issues, are often considered as reasonable fill-ins for when kriging is computationally impractical. 

\pkg{snapKrig} gets its performance edge through restrictions on covariance and point layout, but we nevertheless think it has wide applicability. Users can simply snap irregular points to a reasonably large grid, as we did in the Meuse example. And while some important covariance functions like the 2-dimensional Matérn are excluded, the alternatives provided will be flexible enough approximations in many cases [@koch2020computationally]. These alternatives may also prove to be better predictive models than conventional alternatives in some situations, as we saw in the [Model Comparisons] section.
